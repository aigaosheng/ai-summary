{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ba429e27-7ca0-4366-b4f0-141f0d9c3733",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "SINGAPORE – Premiums for Singaporeans’ basic health insurance plan look set to increase, as MediShield Life is made to work harder, provide more assurance against large medical bills, and pay for new ground-breaking treatments.\n",
    "\n",
    "Recommendations by an expert panel on the review of the scheme are expected by the second half of 2024, said Health Minister Ong Ye Kung during the debate on his ministry’s budget on March 6.\n",
    "\n",
    "MediShield Life is a basic, mandatory health insurance plan launched in November 2015 to protect all Singaporeans against large medical bills for life, regardless of pre-existing conditions.\n",
    "\n",
    "\n",
    "It helps Singaporeans pay for large bills in B2 and C wards, which are subsidised wards, and is meant to cover nine in 10 subsidised bills.\n",
    "\n",
    "But that is no longer the case today.\n",
    "\n",
    "“This nine-in-10 benchmark is being eroded, because hospital bills are getting larger and larger,” said Mr Ong.\n",
    "\n",
    "\n",
    "Bill sizes have grown by 5 per cent annually in public hospitals, and by 7 per cent annually in private hospitals over the last few years, he said.\n",
    "\n",
    "\n",
    "Yes, I would also like to receive SPH Media Group's SPH Media Limited, its related corporations and affiliates as well as their agents and authorised service providers. marketing and promotions.\n",
    "As a result, the proportion of subsidised bills adequately covered by MediShield Life has come down to around eight out of 10, and is expected to slip further.\n",
    "\n",
    "The practical impact of that is subsidised patients are seeing unexpectedly large hospital bills, he pointed out. After the subsidy and MediShield Life, there is still a substantial out-of-pocket component left.\n",
    "\n",
    "\n",
    "This is when higher healthcare costs start to bite.\n",
    "\n",
    "\n",
    "MOH has, therefore, tasked the 11-member MediShield Life Council to comprehensively review the scheme, said Mr Ong.\n",
    "\n",
    "The last time it was reviewed was in 2020, when premiums went up by 25 per cent on average.\n",
    "\n",
    "MOH had said in 2019 that it will review claim limits for MediShield Life coverage every three years or so.\n",
    "\n",
    "The MediShield Life Council is headed by Mrs Fang Ai Lian, the former chairwoman of Ernst & Young. It has already begun the review process and will look into three key areas.\n",
    "\n",
    "First, the council will look into enhancing MediShield Life to give Singaporeans greater assurance against large bills. This means increasing the claim limits – how much a patient can claim from MediShield Life – for both surgical operations and hospital stays.\n",
    "\n",
    "“We envisage a fairly significant increase in the claim limits. For example, for an episode involving angioplasty, where a stent is placed into your heart to open a blocked artery, plus, say, a few nights of ICU stay, the claim limits may need to double. This will reduce out-of-pocket costs significantly,” said Mr Ong.\n",
    "\n",
    "Second, the council will look at enhancing other outpatient coverage.\n",
    "\n",
    "Mr Ong said that there is a need to raise the claim limits for treatments such as kidney dialysis to reduce out-of-pocket expenses for patients.\n",
    "\n",
    "The council will also explore extending coverage to more types of outpatient care.\n",
    "\n",
    "He pointed out that some of the most costly outpatient treatments are for cancer.\n",
    "\n",
    "MOH is facing an especially difficult challenge with cancer, he noted, with accelerating treatment costs.\n",
    "\n",
    "The ministry had earlier reviewed cancer drug financing and introduced changes that allowed the Government to negotiate lower prices.\n",
    "\n",
    "Lastly, the council will also consider expanding MediShield Life coverage to new ground-breaking treatments, specifically cell, tissue and gene therapy products, also known as CTGTPs.\n",
    "\n",
    "According to the World Health Organisation (WHO), CTGTPs include medicinal products like haematopoietic stem cells, skin grafts, novel gene editing technologies and engineered tissues.\n",
    "\n",
    "WHO said that their use in the treatment of human diseases and physical conditions has captured wide interest due to their potential to address many unmet medical needs.\n",
    "\n",
    "Mr Ong pointed out that with medical science advancing rapidly, CTGTPs have the potential to revolutionise healthcare and deliver effective treatment of previously incurable diseases. Some describe it as the equivalent of a moonshot in healthcare, he said.\n",
    "\n",
    "He explained that, essentially, the one-time treatment involves extracting the patient’s blood, teaching and equipping the cells in the blood to target and kill cancer cells, for instance, and then putting the cells back into the patient’s body to do their work.\n",
    "\n",
    "“However, while the technology is promising and advancing fast, it is nascent and very expensive. It could cost anything from a few hundred thousand dollars to a few million dollars per treatment,” said Mr Ong.\n",
    "\n",
    "“We want to start including CTGTPs under MediShield Life coverage. But we need to put in place safeguards to ensure that financing of CTGTPs is sustainable.\n",
    "\n",
    "“For instance, we will need to extend MediShield Life coverage only to treatments that are assessed to be safe, clinically effective and cost-effective,” said Mr Ong.\n",
    "\n",
    "In other words, if a treatment costs a few million dollars, with a small chance of curing a small group of people, it is not cost-effective, said Mr Ong.\n",
    "\n",
    "He added: “This is a significant step to help all Singaporean patients, regardless of their income levels, have access to cost-effective, novel, state-of-the-art therapies.”\n",
    "\n",
    "While the proposed changes will better protect subsidised patients against major health episodes, MediShield Life premiums will inevitably go up, he said.\n",
    "\n",
    "“But rest assured that we will do the necessary to ensure that as far as possible, premiums can be paid fully by MediSave,” said Mr Ong.\n",
    "\n",
    "For example, the Government will consider enhancing premium subsidies or MediSave top-ups for specific groups.\n",
    "\n",
    "Singaporeans may have to use more of their medical savings in MediSave for small hospital bills so that MediShield Life can better focus on big hospital bills, which will then moderate premium increases, said Mr Ong.\n",
    "\n",
    "“No one will lose MediShield Life coverage due to a genuine inability to afford the premiums,” Mr Ong stressed.\n",
    "\n",
    "MOH will share more details when the council completes its review in the second half of 2024.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bfe82dd5-d876-4324-a16d-f8b10b9c9071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 164 tensors from /home/gs/hf_home/models/models--google--gemma-2b-it/gemma-2b-it.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                          gemma.block_count u32              = 18\n",
      "llama_model_loader: - kv   4:                     gemma.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1\n",
      "llama_model_loader: - kv   8:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv   9:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  14:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256128]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256128]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256128]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - type  f32:  164 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 544/256128 vs 388/256128 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256128\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_head           = 8\n",
      "llm_load_print_meta: n_head_kv        = 1\n",
      "llm_load_print_meta: n_layer          = 18\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 256\n",
      "llm_load_print_meta: n_embd_v_gqa     = 256\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 16384\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 2B\n",
      "llm_load_print_meta: model ftype      = all F32 (guessed)\n",
      "llm_load_print_meta: model params     = 2.51 B\n",
      "llm_load_print_meta: model size       = 9.34 GiB (32.00 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.06 MiB\n",
      "llm_load_tensors:        CPU buffer size =  9561.29 MiB\n",
      ".............................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    36.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   36.00 MiB, K (f16):   18.00 MiB, V (f16):   18.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     9.02 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   504.25 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '1', 'general.architecture': 'gemma', 'gemma.feed_forward_length': '16384', 'gemma.attention.head_count': '8', 'general.name': 'gemma-2b-it', 'gemma.context_length': '8192', 'gemma.block_count': '18', 'gemma.embedding_length': '2048', 'gemma.attention.head_count_kv': '1', 'gemma.attention.key_length': '256', 'tokenizer.ggml.model': 'llama', 'gemma.attention.value_length': '256', 'gemma.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.bos_token_id': '2'}\n"
     ]
    }
   ],
   "source": [
    "#test llama2\n",
    "from llama_cpp import Llama\n",
    "import time\n",
    "\n",
    "llm = Llama(\n",
    "      # model_path=\"/home/gs/work/llama.cpp/models/llama2-7b/ggml-model-f16.gguf\",\n",
    "    model_path=\"/home/gs/hf_home/models/models--google--gemma-2b-it/gemma-2b-it.gguf\",\n",
    "    # model_path=\"/home/gs/hf_home/models/models--google--gemma-2b/gemma-2b.gguf\",\n",
    "    chat_format=\"gemma\", #\"llama-2\", #\n",
    "      # n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
    "      # seed=1337, # Uncomment to set a specific seed\n",
    "      n_ctx=2048, # Uncomment to increase the context window\n",
    ")\n",
    "st_tm = time.time()\n",
    "# output = llm(\n",
    "#       f\"Q: You are a professional English teacher. Please assist me summary the following document: {doc}. A: \", # Prompt\n",
    "#       max_tokens=100, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "#       stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "#       echo=False # Echo the prompt back in the output\n",
    "# ) # Generate a completion, can also call create_completion\n",
    "# print(f\"*** running time = {time.time() - st_tm}\")\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "40443c6f-cd07-4900-9e7b-4f08e324a108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     449.26 ms\n",
      "llama_print_timings:      sample time =      25.97 ms /    23 runs   (    1.13 ms per token,   885.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =   26599.19 ms /  1301 tokens (   20.45 ms per token,    48.91 tokens per second)\n",
      "llama_print_timings:        eval time =    3015.07 ms /    22 runs   (  137.05 ms per token,     7.30 tokens per second)\n",
      "llama_print_timings:       total time =   29863.67 ms /  1323 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MediShield Life Council is headed by Mrs Fang Ai Lian, the former chairwoman of Ernst & Young.\n"
     ]
    }
   ],
   "source": [
    "# print(output[\"choices\"][0][\"text\n",
    "output2 = llm.create_chat_completion(\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": f\"You are an assistant to help summarize the document.\"},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": f\"who headed MediShield Life Council according to the document : {doc}? \"\n",
    "          }\n",
    "      ],\n",
    "    max_tokens=200,\n",
    "    temperature=0.0,\n",
    "    # stop=[\".\", \"\\n\"],\n",
    ")\n",
    "# output2 = llm(\n",
    "#       f\"Explain the key topics in the  document: {doc}\", # Prompt\n",
    "#       max_tokens=200, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "#       stop=[\".\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "#       echo=False # Echo the prompt back in the output\n",
    "# ) \n",
    "print(output2[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cc28db75-f484-4c18-b086-2e35cd45656e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"explain key topics in the document: \\nSINGAPORE – Premiums for Singaporeans’ basic health insurance plan look set to increase, as MediShield Life is made to work harder, provide more assurance against large medical bills, and pay for new ground-breaking treatments.\\n\\nRecommendations by an expert panel on the review of the scheme are expected by the second half of 2024, said Health Minister Ong Ye Kung during the debate on his ministry’s budget on March 6.\\n\\nMediShield Life is a basic, mandatory health insurance plan launched in November 2015 to protect all Singaporeans against large medical bills for life, regardless of pre-existing conditions.\\n\\n\\nIt helps Singaporeans pay for large bills in B2 and C wards, which are subsidised wards, and is meant to cover nine in 10 subsidised bills.\\n\\nBut that is no longer the case today.\\n\\n“This nine-in-10 benchmark is being eroded, because hospital bills are getting larger and larger,” said Mr Ong.\\n\\n\\nBill sizes have grown by 5 per cent annually in public hospitals, and by 7 per cent annually in private hospitals over the last few years, he said.\\n\\n\\nYes, I would also like to receive SPH Media Group's SPH Media Limited, its related corporations and affiliates as well as their agents and authorised service providers. marketing and promotions.\\nAs a result, the proportion of subsidised bills adequately covered by MediShield Life has come down to around eight out of 10, and is expected to slip further.\\n\\nThe practical impact of that is subsidised patients are seeing unexpectedly large hospital bills, he pointed out. After the subsidy and MediShield Life, there is still a substantial out-of-pocket component left.\\n\\n\\nThis is when higher healthcare costs start to bite.\\n\\n\\nMOH has, therefore, tasked the 11-member MediShield Life Council to comprehensively review the scheme, said Mr Ong.\\n\\nThe last time it was reviewed was in 2020, when premiums went up by 25 per cent on average.\\n\\nMOH had said in 2019 that it will review claim limits for MediShield Life coverage every three years or so.\\n\\nThe MediShield Life Council is headed by Mrs Fang Ai Lian, the former chairwoman of Ernst & Young. It has already begun the review process and will look into three key areas.\\n\\nFirst, the council will look into enhancing MediShield Life to give Singaporeans greater assurance against large bills. This means increasing the claim limits – how much a patient can claim from MediShield Life – for both surgical operations and hospital stays.\\n\\n“We envisage a fairly significant increase in the claim limits. For example, for an episode involving angioplasty, where a stent is placed into your heart to open a blocked artery, plus, say, a few nights of ICU stay, the claim limits may need to double. This will reduce out-of-pocket costs significantly,” said Mr Ong.\\n\\nSecond, the council will look at enhancing other outpatient coverage.\\n\\nMr Ong said that there is a need to raise the claim limits for treatments such as kidney dialysis to reduce out-of-pocket expenses for patients.\\n\\nThe council will also explore extending coverage to more types of outpatient care.\\n\\nHe pointed out that some of the most costly outpatient treatments are for cancer.\\n\\nMOH is facing an especially difficult challenge with cancer, he noted, with accelerating treatment costs.\\n\\nThe ministry had earlier reviewed cancer drug financing and introduced changes that allowed the Government to negotiate lower prices.\\n\\nLastly, the council will also consider expanding MediShield Life coverage to new ground-breaking treatments, specifically cell, tissue and gene therapy products, also known as CTGTPs.\\n\\nAccording to the World Health Organisation (WHO), CTGTPs include medicinal products like haematopoietic stem cells, skin grafts, novel gene editing technologies and engineered tissues.\\n\\nWHO said that their use in the treatment of human diseases and physical conditions has captured wide interest due to their potential to address many unmet medical needs.\\n\\nMr Ong pointed out that with medical science advancing rapidly, CTGTPs have the potential to revolutionise healthcare and deliver effective treatment of previously incurable diseases. Some describe it as the equivalent of a moonshot in healthcare, he said.\\n\\nHe explained that, essentially, the one-time treatment involves extracting the patient’s blood, teaching and equipping the cells in the blood to target and kill cancer cells, for instance, and then putting the cells back into the patient’s body to do their work.\\n\\n“However, while the technology is promising and advancing fast, it is nascent and very expensive. It could cost anything from a few hundred thousand dollars to a few million dollars per treatment,” said Mr Ong.\\n\\n“We want to start including CTGTPs under MediShield Life coverage. But we need to put in place safeguards to ensure that financing of CTGTPs is sustainable.\\n\\n“For instance, we will need to extend MediShield Life coverage only to treatments that are assessed to be safe, clinically effective and cost-effective,” said Mr Ong.\\n\\nIn other words, if a treatment costs a few million dollars, with a small chance of curing a small group of people, it is not cost-effective, said Mr Ong.\\n\\nHe added: “This is a significant step to help all Singaporean patients, regardless of their income levels, have access to cost-effective, novel, state-of-the-art therapies.”\\n\\nWhile the proposed changes will better protect subsidised patients against major health episodes, MediShield Life premiums will inevitably go up, he said.\\n\\n“But rest assured that we will do the necessary to ensure that as far as possible, premiums can be paid fully by MediSave,” said Mr Ong.\\n\\nFor example, the Government will consider enhancing premium subsidies or MediSave top-ups for specific groups.\\n\\nSingaporeans may have to use more of their medical savings in MediSave for small hospital bills so that MediShield Life can better focus on big hospital bills, which will then moderate premium increases, said Mr Ong.\\n\\n“No one will lose MediShield Life coverage due to a genuine inability to afford the premiums,” Mr Ong stressed.\\n\\nMOH will share more details when the council completes its review in the second half of 2024.\\n. \""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"explain key topics in the document: {doc}. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "118381c2-4ad0-4041-b108-90de9a2120c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 164 tensors from /home/gs/hf_home/models/models--google--gemma-2b/gemma-2b.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2b\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                          gemma.block_count u32              = 18\n",
      "llama_model_loader: - kv   4:                     gemma.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1\n",
      "llama_model_loader: - kv   8:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv   9:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  14:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256128]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256128]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256128]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - type  f32:  164 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 544/256128 vs 388/256128 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256128\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_head           = 8\n",
      "llm_load_print_meta: n_head_kv        = 1\n",
      "llm_load_print_meta: n_layer          = 18\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 256\n",
      "llm_load_print_meta: n_embd_v_gqa     = 256\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 16384\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 2B\n",
      "llm_load_print_meta: model ftype      = all F32 (guessed)\n",
      "llm_load_print_meta: model params     = 2.51 B\n",
      "llm_load_print_meta: model size       = 9.34 GiB (32.00 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2b\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.06 MiB\n",
      "llm_load_tensors:        CPU buffer size =  9561.29 MiB\n",
      ".............................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB\n",
      "llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     6.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   504.25 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '1', 'general.architecture': 'gemma', 'gemma.feed_forward_length': '16384', 'gemma.attention.head_count': '8', 'general.name': 'gemma-2b', 'gemma.context_length': '8192', 'gemma.block_count': '18', 'gemma.embedding_length': '2048', 'gemma.attention.head_count_kv': '1', 'gemma.attention.key_length': '256', 'tokenizer.ggml.model': 'llama', 'gemma.attention.value_length': '256', 'gemma.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.bos_token_id': '2'}\n",
      "\n",
      "llama_print_timings:        load time =     158.65 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     158.58 ms /     3 tokens (   52.86 ms per token,    18.92 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     157.92 ms /     4 tokens\n",
      "\n",
      "llama_print_timings:        load time =     158.65 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     201.31 ms /     8 tokens (   25.16 ms per token,    39.74 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     200.99 ms /     9 tokens\n"
     ]
    }
   ],
   "source": [
    "llm_emb = Llama(\n",
    "      # model_path=\"/home/gs/work/llama.cpp/models/llama2-7b/ggml-model-f16.gguf\",\n",
    "    # model_path=\"/home/gs/hf_home/models/models--google--gemma-2b-it/gemma-2b-it.gguf\",\n",
    "    model_path=\"/home/gs/hf_home/models/models--google--gemma-2b/gemma-2b.gguf\",\n",
    "    chat_format=\"gemma\", #\"llama-2\",\n",
    "      # n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
    "      # seed=1337, # Uncomment to set a specific seed\n",
    "      # n_ctx=2048, # Uncomment to increase the context window\n",
    "    embedding=True, \n",
    ")\n",
    "x = llm_emb.embed(\"welcome paris\", normalize=True, truncate=True, return_count=False)\n",
    "y = llm_emb.embed(\"beijing is capital of china.\", normalize=True, truncate=True, return_count=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92ce9e1d-35a8-4950-b0cf-b4f2135688b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7774497818888593"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(lambda a: a[0]*a[1], zip(x,y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e23ad0f-e2f7-44da-9478-c31822f2fe4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': 0, 'delta': {'role': 'assistant'}, 'finish_reason': None}\n",
      "{'index': 0, 'delta': {'content': 'Beijing'}, 'finish_reason': None}\n",
      "{'index': 0, 'delta': {'content': ' is'}, 'finish_reason': None}\n",
      "{'index': 0, 'delta': {'content': ' the'}, 'finish_reason': None}\n",
      "{'index': 0, 'delta': {'content': ' capital'}, 'finish_reason': None}\n",
      "{'index': 0, 'delta': {'content': ' city'}, 'finish_reason': None}\n",
      "{'index': 0, 'delta': {'content': ' of'}, 'finish_reason': None}\n",
      "{'index': 0, 'delta': {'content': ' China'}, 'finish_reason': None}\n",
      "{'index': 0, 'delta': {'content': '.'}, 'finish_reason': None}\n",
      "{'index': 0, 'delta': {'content': ' It'}, 'finish_reason': None}\n",
      "{'index': 0, 'delta': {'content': ' is'}, 'finish_reason': None}\n",
      "{'index': 0, 'delta': {'content': ' located'}, 'finish_reason': None}\n",
      "{'index': 0, 'delta': {'content': ' in'}, 'finish_reason': None}\n",
      "{'index': 0, 'delta': {'content': ' the'}, 'finish_reason': None}\n",
      "{'index': 0, 'delta': {'content': ' north'}, 'finish_reason': None}\n",
      "{'index': 0, 'delta': {'content': '-'}, 'finish_reason': None}\n",
      "{'index': 0, 'delta': {'content': 'central'}, 'finish_reason': None}\n",
      "{'index': 0, 'delta': {'content': ' part'}, 'finish_reason': None}\n",
      "{'index': 0, 'delta': {'content': ' of'}, 'finish_reason': None}\n",
      "{'index': 0, 'delta': {'content': ' the'}, 'finish_reason': None}\n",
      "{'index': 0, 'delta': {'content': ' country'}, 'finish_reason': None}\n",
      "{'index': 0, 'delta': {'content': '.'}, 'finish_reason': None}\n",
      "{'index': 0, 'delta': {}, 'finish_reason': 'stop'}\n",
      "['Beijing', ' is', ' the', ' capital', ' city', ' of', ' China', '.', ' It', ' is', ' located', ' in', ' the', ' north', '-', 'central', ' part', ' of', ' the', ' country', '.']\n",
      "*** running time = 3.363513469696045\n"
     ]
    }
   ],
   "source": [
    "#test google gemmas\n",
    "from llama_cpp import Llama\n",
    "import time\n",
    "\n",
    "llm = Llama(\n",
    "      # model_path=\"/home/gs/work/llama.cpp/models/llama2-7b/ggml-model-f16.gguf\",\n",
    "    model_path=\"/home/gs/hf_home/models/models--google--gemma-2b-it/gemma-2b-it.gguf\",\n",
    "     chat_format=\"gemma\", #\"llama-2\",\n",
    "      # n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
    "      # seed=1337, # Uncomment to set a specific seed\n",
    "      # n_ctx=2048, # Uncomment to increase the context window\n",
    "    verbose = False,\n",
    ")\n",
    "st = time.time()\n",
    "result = llm.create_chat_completion(\n",
    "    stream=True,\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": \"You are a coach assistant \"},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"where is beijing?\", #\"List names of planets in the solar system\"\n",
    "          }\n",
    "      ]\n",
    ")\n",
    "ss = []\n",
    "for v in result:\n",
    "    # print(v)\n",
    "    for vv in v['choices']:\n",
    "        print(vv)\n",
    "        cc = vv['delta'].get('content', None)        \n",
    "        if cc:\n",
    "            # print(cc)\n",
    "            ss.append(cc)  \n",
    "        # print(v[0])\n",
    "    # break\n",
    "print(ss)\n",
    "print(f\"*** running time = {time.time() - st}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3e3f8ff-6fae-4b8f-9abd-8bde29ed75e8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a breakdown of the Singapore PLSE (Public Sector Employee Licensing Scheme) score criteria:\n",
      "**Overall Score:**\n",
      "* The PLSE score ranges from 10 to 90,\n",
      " with higher scores indicating greater competency and suitability for public sector jobs.\n",
      "* A score of 80 or above is considered high,\n",
      " while a score below 80 is considered low.\n",
      "**Sub-scores:**\n",
      "* **Knowledge and Skills Assessment (KSA):\n",
      "** This section assesses knowledge and skills relevant to the specific job profile.\n",
      " It consists of multiple-choice questions and practical assessments.\n",
      "* **Situational Judgement Test (SJT):** This section assesses the candidate's ability to handle real-life situations and decision-making skills.\n",
      "* **Personal Qualities Assessment (PQA):** This section assesses the candidate's personality traits,\n",
      " work ethic, and motivation.\n",
      "**Sub-score Ranges:**\n",
      "| Sub-score Range | Description |\n",
      "|---|---|\n",
      "| 10-25 | Low |\n",
      "| 26-45 | Moderate |\n",
      "| 46-65 | High |\n",
      "| 66-85 | Very high |\n",
      "| 86-90 | Exceptional |\n",
      "**Factors Affecting Scores:**\n",
      "* **Age and experience:** Younger candidates may have lower scores due to ongoing education and training.\n",
      "* **Education and training:** Candidates with relevant qualifications and certifications may have higher scores.\n",
      "* **Work experience:** Candidates with previous public sector experience may have higher scores.\n",
      "* **Language proficiency:** Candidates who speak multiple languages may have higher scores in the SJT.\n",
      "* **Personal qualities:** Strong personality traits and work ethic can contribute to a high score.\n",
      "**Note:**\n",
      "* The PLSE score is a requirement for all public sector recruitment processes.\n",
      "* Candidates must achieve a minimum score to be considered for appointment.\n",
      "* The PLSE score is not the only factor considered in recruitment decisions.\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import time\n",
    "# import streamlit as st\n",
    "\n",
    "llm_model_pth = \"/home/gs/hf_home/models/models--google--gemma-2b-it/gemma-2b-it.gguf\"\n",
    "llm_chat_format = \"gemma\"\n",
    "\n",
    "#chunk version\n",
    "MAX_SENTENCE_LIMIT = 5 \n",
    "def chat_llm_chunk(msg, is_chunk = False):\n",
    "    # @st.cache_resource\n",
    "    def  load_llm():\n",
    "        llm = Llama(\n",
    "            # model_path=\"/home/gs/work/llama.cpp/models/llama2-7b/ggml-model-f16.gguf\",\n",
    "            model_path=llm_model_pth,\n",
    "            chat_format=llm_chat_format, #\"llama-2\",\n",
    "            verbose = False,\n",
    "            # n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
    "            # seed=1337, # Uncomment to set a specific seed\n",
    "            # n_ctx=2048, # Uncomment to increase the context window\n",
    "        )\n",
    "        return llm\n",
    "        \n",
    "    llm = load_llm()\n",
    "    # print(f\"**** {llm}\")\n",
    "    result = llm.create_chat_completion(\n",
    "        stream = True,\n",
    "        messages = msg,\n",
    "        seed = 100,\n",
    "    )\n",
    "    # print(result)\n",
    "    if is_chunk:\n",
    "        sen = \"\"\n",
    "        for v in result:\n",
    "            for vv in v['choices']:\n",
    "                cc = vv['delta'].get('content', None)  \n",
    "                if cc:\n",
    "                    # cc = cc.strip(\"*\")\n",
    "                    if cc.startswith(\"\\n\"):\n",
    "                        if len(sen.strip()):\n",
    "                            yield sen\n",
    "                            sen = \"\"\n",
    "                    else:\n",
    "                        sen += cc\n",
    "                        if len(sen.split(\" \")) > MAX_SENTENCE_LIMIT:\n",
    "                            if cc[0] in (\",.?!:\") or cc[-1] in (\",.?!:\"):\n",
    "                                yield sen\n",
    "                                sen = \"\"\n",
    "        if len(sen.strip()):\n",
    "            yield sen\n",
    "    else:\n",
    "        # sen = \"\"\n",
    "        # for v in result:\n",
    "        #     # print(v)\n",
    "        #     for vv in v['choices']:\n",
    "        #         cc = vv['delta'].get('content', None)  \n",
    "        #         if cc:\n",
    "        #             sen += cc\n",
    "        # yield sen\n",
    "\n",
    "        sen = \"\"\n",
    "        for v in result:\n",
    "            for vv in v['choices']:\n",
    "                cc = vv['delta'].get('content', None)  \n",
    "                yield cc\n",
    "\n",
    "msg = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a coach assistant \"},\n",
    "  {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Explain Singapore PLSE score criteria.\", #\"List names of planets in the solar system\" #\"where is beijing?\", #\"explain the leadship by key points\", #\n",
    "  }\n",
    "]\n",
    "\n",
    "res = chat_llm_chunk(msg, True)\n",
    "\n",
    "# print(list(res))\n",
    "for v in res:\n",
    "    print(v)\n",
    "# from RealtimeTTS import TextToAudioStream, WhisperEngine\n",
    "\n",
    "# engine = WhisperEngine() # replace with your TTS engine\n",
    "# stream = TextToAudioStream(engine)\n",
    "# stream.feed(res)\n",
    "# stream.play_async()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7881fa9d-15e5-4b52-bfd1-7d7e18aaa2b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = \":**\"\n",
    "x.strip(\"*\") in (\",.?!:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a8b44a6-b1a4-4163-8462-40f178b9c1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gs/miniconda3/envs/t2v/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/gs/miniconda3/envs/t2v/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "中国首都在北京。北京是中国首都，是华北地区政治、经济、文化中心。\n",
      "** running time = 0.6446280479431152\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import transformers\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "# login(\"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\")\n",
    "\n",
    "def parse_response(i_txt):\n",
    "    txt = i_txt.split(\"<end_of_turn>\")[-1].replace(\"<eos>\", \"\")\n",
    "    return txt\n",
    "    \n",
    "# model_id = \"gg-hf/gemma-2b-it\"\n",
    "model_id = \"/home/gs/hf_home/models/models--google--gemma-2b-it\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "# quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=dtype,\n",
    "    # quantization_config=quantization_config,\n",
    ")\n",
    "st = time.time()\n",
    "chat = [\n",
    "    { \"role\": \"user\", \"content\": \"中国首都在哪里 ?\" },\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "inputs = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=150)\n",
    "output_txt = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(parse_response(output_txt))\n",
    "print(f\"** running time = {time.time() - st}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f299e78-77a7-4a99-b0d7-bdade863aaa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>user\\nwhat is leadership?<end_of_turn>\\n<start_of_turn>model\\n**Leadership** is the ability to inspire, motivate, and guide a group of people towards a common goal. It involves the following key elements:\\n\\n**1. Vision and Strategy:**\\n- Leaders have a clear vision of the future state they want to achieve and develop a strategic plan to get there.\\n\\n**2. Communication:**\\n- Effective communication is essential for sharing ideas, building trust, and engaging followers.\\n\\n**3. Influence:**\\n- Leaders can influence others through persuasion, negotiation, and setting a positive example.\\n\\n**4. Decision-Making:**\\n- Leaders make informed decisions that align with the vision and strategy.\\n\\n**5. Accountability:**\\n- Leaders hold themselves and others accountable for achieving goals and adhering to the'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e68b2c3a-1f71-4cf9-b7ff-778a4a2dae09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:02<00:00,  1.16s/it]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>List names of planets in the solar system in order from the sun, using the scientific method.\n",
      "\n",
      "A 100-W lightbulb is plugged into a standard $120-\\mathrm{V}$ (rms) outlet. Find $(a) I_{\\text {mas }}(b) I_{\\max }$, and $(c)$ the maximum power.\n",
      "\n",
      "A 100-turn, 2.0-cm-diameter coil is at rest with its axis vertical. A uniform magnetic field $60^{\\circ}$ away from vertical increases from 0.50 T to 1.50 T in 0.60 s. What is the induced emf in the coil?\n",
      "\n",
      "A 100-turn coil has a radius of 10.0 cm. The coil is placed in a spatially uniform magnetic field of magnitude 0.500 T so that the face of the coil and the magnetic field are perpendicular. The coil is removed from the magnetic field in 0.100 s. What is the average emf induced in the coil as it is removed from the magnetic field?<eos>\n"
     ]
    }
   ],
   "source": [
    "# pip install accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"/home/gs/hf_home/models/models--google--gemma-2b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "\n",
    "input_text = \"List names of planets in the solar system\" #\"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens = 256)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e519754-70c8-4d00-8e79-63bfe4f4ba1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gs/miniconda3/envs/t2v/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/gs/miniconda3/envs/t2v/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/home/gs/miniconda3/envs/t2v/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "ALSA lib pcm_route.c:877:(find_matching_chmap) Found no matching channel map\n",
      "ALSA lib pcm_route.c:877:(find_matching_chmap) Found no matching channel map\n",
      "ALSA lib pcm_route.c:877:(find_matching_chmap) Found no matching channel map\n",
      "ALSA lib pcm_route.c:877:(find_matching_chmap) Found no matching channel map\n",
      "ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='100' class='' max='749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      13.35% [100/749 00:18&lt;01:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-24 23:35:31,082] [0/0] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='302' class='' max='302' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [302/302 00:20&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-24 23:35:49,397] [1/0] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored\n",
      "[2024-02-24 23:35:49,400] [1/0] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored\n",
      "[2024-02-24 23:35:56,490] [1/0] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored\n",
      "[2024-02-24 23:35:56,505] [1/0] torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='5' class='' max='749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.67% [5/749 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='77' class='' max='77' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [77/77 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='5' class='' max='749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.67% [5/749 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='152' class='' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [152/152 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='5' class='' max='749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.67% [5/749 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='152' class='' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [152/152 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='5' class='' max='749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.67% [5/749 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='152' class='' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [152/152 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='5' class='' max='749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.67% [5/749 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='77' class='' max='77' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [77/77 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import time\n",
    "import streamlit as st\n",
    "\n",
    "llm_model_pth = \"/home/gs/hf_home/models/models--google--gemma-2b-it/gemma-2b-it.gguf\"\n",
    "llm_chat_format = \"gemma\"\n",
    "MAX_SENTENCE_LIMIT = 5 \n",
    "\n",
    "def chat_llm(msg, is_chunk = False):\n",
    "    # @st.cache_resource\n",
    "    def  load_llm():\n",
    "        llm = Llama(\n",
    "            # model_path=\"/home/gs/work/llama.cpp/models/llama2-7b/ggml-model-f16.gguf\",\n",
    "            model_path=llm_model_pth,\n",
    "            chat_format=llm_chat_format, #\"llama-2\",\n",
    "            verbose = False,\n",
    "            # n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
    "            # seed=1337, # Uncomment to set a specific seed\n",
    "            # n_ctx=2048, # Uncomment to increase the context window\n",
    "        )\n",
    "        return llm\n",
    "        \n",
    "    llm = load_llm()\n",
    "    # print(f\"**** {llm}\")\n",
    "    result = llm.create_chat_completion(\n",
    "        stream = True,\n",
    "        messages = msg,\n",
    "        seed = 100,\n",
    "    )\n",
    "    # print(result)\n",
    "    if is_chunk:\n",
    "        sen = \"\"\n",
    "        for v in result:\n",
    "            for vv in v['choices']:\n",
    "                cc = vv['delta'].get('content', None)  \n",
    "                if cc:\n",
    "                    if cc.startswith(\"\\n\"):\n",
    "                        if len(sen.strip()):\n",
    "                            yield sen\n",
    "                            sen = \"\"\n",
    "                    else:\n",
    "                        sen += cc\n",
    "                        if len(sen.split(\" \")) > MAX_SENTENCE_LIMIT:\n",
    "                            if cc in (\",.?!:\"):\n",
    "                                yield sen\n",
    "                                sen = \"\"\n",
    "        if len(sen.strip()):\n",
    "            yield sen\n",
    "    else:\n",
    "        sen = \"\"\n",
    "        for v in result:\n",
    "            # print(v)\n",
    "            for vv in v['choices']:\n",
    "                cc = vv['delta'].get('content', None)  \n",
    "                if cc:\n",
    "                    sen += cc\n",
    "        yield sen\n",
    "\n",
    "msg = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a coach assistant \"},\n",
    "  {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"List names of planets in the solar system\" #\"where is beijing?\", #\n",
    "  }\n",
    "]\n",
    "\n",
    "res = chat_llm(msg, True)\n",
    "# for v in res:\n",
    "#     print(v)\n",
    "from RealtimeTTS import TextToAudioStream, WhisperEngine\n",
    "\n",
    "engine = WhisperEngine() # replace with your TTS engine\n",
    "stream = TextToAudioStream(engine)\n",
    "stream.feed(res)\n",
    "stream.play_async()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13e5e0a1-1494-41b9-adf7-47687eda55ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='5' class='' max='749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.67% [5/749 00:00&lt;00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='302' class='' max='302' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [302/302 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='749' class='' max='749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [749/749 00:01&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='2248' class='' max='2248' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [2248/2248 00:08&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib pcm.c:8568:(snd_pcm_recover) underrun occurred\n"
     ]
    }
   ],
   "source": [
    "msg = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a coach assistant \"},\n",
    "  {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"explain the leadship by key points. use 50 words\", #\"List names of planets in the solar system\" #\"where is beijing?\", #\n",
    "  }\n",
    "]\n",
    "\n",
    "res = chat_llm(msg, True)\n",
    "stream.feed(res)\n",
    "stream.play_async()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
