{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "de301b96-ae01-40b9-b5a6-b6c3a1d25ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4360f15b-fe5d-491c-87a6-dee6d9218e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydantic\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1983b5c-ef8c-462f-8dab-2c51665b46ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
    "\n",
    "# template1 = \"\"\"Role: {question}\n",
    "\n",
    "# Content: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3faab5cd-8836-4abf-b06d-84e01875b624",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "root ::= object \n",
      "object ::= [{] ws object_11 [}] \n",
      "value ::= object | array | string | number | boolean | [n] [u] [l] [l] \n",
      "array ::= [[] ws array_15 []] \n",
      "string ::= [\"] string_18 [\"] ws \n",
      "number ::= number_19 number_20 ws \n",
      "boolean ::= boolean_21 ws \n",
      "ws ::= ws_23 \n",
      "object_8 ::= string [:] ws value object_10 \n",
      "object_9 ::= [,] ws string [:] ws value \n",
      "object_10 ::= object_9 object_10 | \n",
      "object_11 ::= object_8 | \n",
      "array_12 ::= value array_14 \n",
      "array_13 ::= [,] ws value \n",
      "array_14 ::= array_13 array_14 | \n",
      "array_15 ::= array_12 | \n",
      "string_16 ::= [^\"\\] | [\\] string_17 \n",
      "string_17 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "string_18 ::= string_16 string_18 | \n",
      "number_19 ::= [-] | \n",
      "number_20 ::= [0-9] number_20 | [0-9] \n",
      "boolean_21 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] \n",
      "ws_22 ::= [ <U+0009><U+000A>] ws \n",
      "ws_23 ::= ws_22 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make sure the model path is correct for your system!\n",
    "n_gpu_layers =  -1\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/home/gs/hf_home/models/models--google--gemma-2b-it/gemma-2b-it.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    callback_manager=callback_manager,\n",
    "    # n_ctx=2048, # Uncomment to increase the context window\n",
    "    # temperature=0.75,\n",
    "    # f16_kv=True,\n",
    "    verbose=False,  # Verbose is required to pass to the callback manager\n",
    "    grammar_path = \"/home/gs/work/ai-summary/json.gbnf\",\n",
    "    # grammar_path = \"/home/gs/work/ai-summary/list.gbnf\",\n",
    ")\n",
    "# llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b657b97a-866f-486a-92a6-cbb7cd962492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 164 tensors from /home/gs/hf_home/models/models--google--gemma-2b-it/gemma-2b-it.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2b-it\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                          gemma.block_count u32              = 18\n",
      "llama_model_loader: - kv   4:                     gemma.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1\n",
      "llama_model_loader: - kv   8:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv   9:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  14:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256128]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256128]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256128]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - type  f32:  164 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 544/256128 vs 388/256128 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256128\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_head           = 8\n",
      "llm_load_print_meta: n_head_kv        = 1\n",
      "llm_load_print_meta: n_layer          = 18\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 256\n",
      "llm_load_print_meta: n_embd_v_gqa     = 256\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 16384\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 2B\n",
      "llm_load_print_meta: model ftype      = all F32 (guessed)\n",
      "llm_load_print_meta: model params     = 2.51 B\n",
      "llm_load_print_meta: model size       = 9.34 GiB (32.00 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.06 MiB\n",
      "llm_load_tensors:        CPU buffer size =  9561.29 MiB\n",
      ".............................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB\n",
      "llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     6.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   504.25 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '1', 'general.architecture': 'gemma', 'gemma.feed_forward_length': '16384', 'gemma.attention.head_count': '8', 'general.name': 'gemma-2b-it', 'gemma.context_length': '8192', 'gemma.block_count': '18', 'gemma.embedding_length': '2048', 'gemma.attention.head_count_kv': '1', 'gemma.attention.key_length': '256', 'tokenizer.ggml.model': 'llama', 'gemma.attention.value_length': '256', 'gemma.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.bos_token_id': '2'}\n",
      "from_string grammar:\n",
      "root ::= [[] items []] EOF \n",
      "items ::= item items_7 \n",
      "EOF ::= [<U+000A>] \n",
      "item ::= string \n",
      "items_4 ::= [,] items_6 item \n",
      "ws ::= [ ] \n",
      "items_6 ::= ws items_6 | \n",
      "items_7 ::= items_4 items_7 | \n",
      "string ::= [\"] word string_12 [\"] string_13 \n",
      "word ::= word_14 \n",
      "string_10 ::= string_11 word \n",
      "string_11 ::= ws string_11 | ws \n",
      "string_12 ::= string_10 string_12 | \n",
      "string_13 ::= ws string_13 | \n",
      "word_14 ::= [a-zA-Z] word_14 | [a-zA-Z] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = -1\n",
    "n_batch = 512\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/home/gs/hf_home/models/models--google--gemma-2b-it/gemma-2b-it.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    # f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    "    grammar_path=\"./list.gbnf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c0fd5e19-f0b1-4e04-b573-caaa783c562a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"q3_2022\": {\n",
      "    \"cars_sold\": \"1,234,567\"\n",
      "  }}"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\\n  \"q3_2022\": {\\n    \"cars_sold\": \"1,234,567\"\\n  }}'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = llm.invoke(\"How many cars did Tesla sell in Q3 2022 in JSON format\")\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8c1ac67-43d6-4d0d-8295-87db4c5544de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**Step 1: Define the JSON object:**\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"name\": \"John\",\n",
      "  \"age\": 30,\n",
      "  \"occupation\": \"Software Engineer\",\n",
      "  \"city\": \"New York\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* `name`: This key stores the person's name as \"John\".\n",
      "* `age`: This key stores the person's age as 30.\n",
      "* `occupation`: This key stores the person's occupation as \"Software Engineer\".\n",
      "* `city`: This key stores the person's city as \"New York\".\n",
      "\n",
      "**Step 2: Explain each key and its value:**\n",
      "\n",
      "* **name**: This key holds a string value \"John\". It defines the person's name.\n",
      "* **age**: This key holds an integer value 30. It represents the person's age.\n",
      "* **occupation**: This key holds a string value \"Software Engineer\". It specifies the person's profession.\n",
      "* **city**: This key holds a string value \"New York\". It indicates the person's city.\n",
      "\n",
      "**Step 3: Validate the JSON format:**\n",
      "\n",
      "To ensure that the JSON format is correct, each"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Question: Describe a person in JSON format.\n",
    "\"\"\"\n",
    "ss = ''\n",
    "for v in llm_chain.invoke(prompt):\n",
    "    if v.startswith(\"\\n\"):\n",
    "        print(ss)\n",
    "        ss = \"\"\n",
    "    else:\n",
    "        ss += v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41d86453-01d1-4def-bf34-4d588c18a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama \n",
    "llm_emb_inst = Llama(\n",
    "      # model_path=\"/home/gs/work/llama.cpp/models/llama2-7b/ggml-model-f16.gguf\",\n",
    "    # model_path=\"/home/gs/hf_home/models/models--google--gemma-2b-it/gemma-2b-it.gguf\",\n",
    "    model_path=\"/home/gs/hf_home/models/models--google--gemma-2b/gemma-2b.gguf\",\n",
    "    chat_format=\"gemma\", #\"llama-2\",\n",
    "      # n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
    "      # seed=1337, # Uncomment to set a specific seed\n",
    "      # n_ctx=2048, # Uncomment to increase the context window\n",
    "    embedding=True, \n",
    "    verbose=False,\n",
    ")\n",
    "# x = llm_emb.embed(\"welcome paris\", normalize=True, truncate=True, return_count=False)\n",
    "# y = llm_emb.embed(\"beijing is capital of china.\", normalize=True, truncate=True, return_count=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d199c147-185d-40ad-ba38-3c1395cf6014",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=llm_emb_inst.embed(\"welcome paris\", normalize=True, truncate=True, return_count=False)\n",
    "y=llm_emb_inst.embed(\"welcome paris\", normalize=True, truncate=True, return_count=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5efbc82d-f5bd-41be-b70a-6fdc065971cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is Experimentation?\"\n",
    "prompt_template = \"\"\"Answer the question as precise as possible using the provided context. If the answer is\n",
    "                    not contained in the context, say \"answer not available in context\" \\n\\n\n",
    "                    Context: \\n {context}?\\n\n",
    "                    Question: \\n {question} \\n\n",
    "                    Answer:\n",
    "                  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05cf55c6-750c-45e2-8739-713ffc3913d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer the question as precise as possible using the provided context. If the answer is\\n                    not contained in the context, say \"answer not available in context\" \\n\\n\\n                    Context: \\n {context}?\\n\\n                    Question: \\n {question} \\n\\n                    Answer:\\n                  '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
